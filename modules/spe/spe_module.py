import streamlit as st
import pandas as pd
import plotly.express as px
from io import BytesIO
from google.oauth2 import service_account
import gspread
import pymongo
from datetime import datetime, timedelta
from config.spe_config import SPE_SETTINGS
from src.utils.database import get_google_credentials
from config.settings import INACTIVE_EVALUATORS, MONGODB_CONFIG
from st_aggrid import AgGrid, GridOptionsBuilder
from st_aggrid.shared import JsCode
import numpy as np
import plotly.graph_objects as go
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

class SPEModule:
    SCOPES = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive'
    ]

    def __init__(self):
        self.client = self._initialize_client()

    @staticmethod
    @st.cache_resource
    def _initialize_client():
        """Inicializar cliente de Google Sheets con cach√©."""
        try:
            credentials = get_google_credentials()
            return gspread.authorize(credentials)
        except Exception as e:
            st.error(f"Error al inicializar el cliente de Google Sheets: {str(e)}")
            return None

    def load_data(self):
        """Cargar datos desde Google Sheets."""
        try:
            if self.client is None:
                st.error("No se pudo inicializar el cliente de Google Sheets")
                return None
                
            sheet = self.client.open_by_key(SPE_SETTINGS['SPREADSHEET_ID']).worksheet(SPE_SETTINGS['WORKSHEET_NAME'])
            return pd.DataFrame(sheet.get_all_records())
        except Exception as e:
            st.error(f"Error al cargar datos de Google Sheets: {str(e)}")
            return None

    def render_module(self):
        """Renderizar el m√≥dulo SPE."""
        data = self.load_data()
        if data is None:
            return

        # Inicializar conexi√≥n MongoDB
        client = self._init_mongodb_connection()
        db = client[MONGODB_CONFIG['database']]
        collection = db[MONGODB_CONFIG['collections']['rankings']]

        tabs = st.tabs([
            "Reporte de Pendientes", 
            "Reporte de Trabajados",
            "Ranking de Expedientes Trabajados",
            "An√°lisis Din√°mico",
            "Predicci√≥n de Ingresos"  # Nueva pesta√±a
        ])
        
        with tabs[0]:
            self.render_pending_report(data)
        with tabs[1]:
            self.render_worked_report(data)
        with tabs[2]:
            self.render_ranking_report(data, collection)
        with tabs[3]:
            self.render_dynamic_analysis(data)
        with tabs[4]:
            self.render_predictive_analysis(data)  # Nuevo m√©todo

    @staticmethod
    @st.cache_resource
    def _init_mongodb_connection():
        """Inicializar conexi√≥n a MongoDB."""
        return pymongo.MongoClient(st.secrets["connections"]["mongodb"]["uri"])

    def render_ranking_report(self, data, collection):
        """Renderizar pesta√±a de ranking de expedientes trabajados."""
        st.header("Ranking de Expedientes Trabajados")

        COLUMNAS = {
            'EVALUADOR': 'EVALUADOR',
            'EXPEDIENTE': 'EXPEDIENTE',
            'FECHA_TRABAJO': 'Fecha_Trabajo'
        }

        # Usar timezone de Peru para las fechas
        fecha_actual = pd.Timestamp.now(tz='America/Lima').date()
        fecha_ayer = fecha_actual - timedelta(days=1)

        # Convertir fecha de trabajo a datetime considerando timezone
        data[COLUMNAS['FECHA_TRABAJO']] = pd.to_datetime(
            data[COLUMNAS['FECHA_TRABAJO']], 
            format='%d/%m/%Y',
            dayfirst=True,
            errors='coerce'
        ).dt.tz_localize('America/Lima')

        # Filtrar datos del d√≠a actual
        data = data[data[COLUMNAS['FECHA_TRABAJO']].dt.date < fecha_actual]

        # Obtener √∫ltima fecha registrada
        ultima_fecha_db = self._get_last_date_from_db(collection)
        ultima_fecha = ultima_fecha_db.date() if ultima_fecha_db else None

        # Obtener datos hist√≥ricos de MongoDB considerando timezone
        registros_historicos = list(collection.find({
            "modulo": "SPE",
            "fecha": {"$lt": pd.Timestamp(fecha_actual, tz='America/Lima')}
        }).sort("fecha", -1))
        
        # Preparar DataFrame hist√≥rico
        df_historico = pd.DataFrame()
        fechas_guardadas = set()
        
        # Procesar registros hist√≥ricos
        if registros_historicos:
            for registro in registros_historicos:
                fecha = pd.Timestamp(registro['fecha'])
                fechas_guardadas.add(fecha.date())
                fecha_str = fecha.strftime('%d/%m')
                df_temp = pd.DataFrame(registro['datos'])
                if not df_temp.empty:
                    evaluador_col = 'EVALUADOR' if 'EVALUADOR' in df_temp.columns else 'evaluador'
                    df_pivot = pd.DataFrame({
                        'EVALUADOR': df_temp[evaluador_col].tolist(),
                        fecha_str: df_temp['cantidad'].tolist()
                    })
                    if df_historico.empty:
                        df_historico = df_pivot
                    else:
                        df_historico = df_historico.merge(
                            df_pivot, on='EVALUADOR', how='outer'
                        )

        # Procesar datos del d√≠a anterior si no est√°n guardados
        datos_ayer = None
        if fecha_ayer not in fechas_guardadas:
            datos_dia_anterior = data[data[COLUMNAS['FECHA_TRABAJO']].dt.date == fecha_ayer]
            if not datos_dia_anterior.empty:
                datos_ayer = datos_dia_anterior.groupby(COLUMNAS['EVALUADOR']).size().reset_index(name='cantidad')
                fecha_str = fecha_ayer.strftime('%d/%m')
                df_pivot = pd.DataFrame({
                    'EVALUADOR': datos_ayer['EVALUADOR'].tolist(),
                    fecha_str: datos_ayer['cantidad'].tolist()
                })
                if df_historico.empty:
                    df_historico = df_pivot
                else:
                    df_historico = df_historico.merge(
                        df_pivot, on='EVALUADOR', how='outer'
                    )

        # Mostrar tabla de ranking
        if not df_historico.empty:
            df_historico = df_historico.fillna(0)
            
            # Ordenar columnas cronol√≥gicamente
            cols_fecha = [col for col in df_historico.columns if col != 'EVALUADOR']
            cols_ordenadas = ['EVALUADOR'] + sorted(
                cols_fecha,
                key=lambda x: pd.to_datetime(x + f"/{datetime.now().year}", format='%d/%m/%Y')
            )
            
            df_historico = df_historico[cols_ordenadas]
            df_historico['Total'] = df_historico.iloc[:, 1:].sum(axis=1)
            df_historico = df_historico.sort_values('Total', ascending=False)
            st.dataframe(df_historico)

        # Mostrar informaci√≥n y botones
        if ultima_fecha_db:
            st.info(f"√öltima fecha registrada en BD: {ultima_fecha.strftime('%d/%m/%Y')}")
        else:
            st.warning("No hay registros en la base de datos")

        # Botones de acci√≥n
        col1, col2 = st.columns(2)

        with col1:
            if datos_ayer is not None and (ultima_fecha is None or ultima_fecha < fecha_ayer):
                if st.button("üíæ Guardar producci√≥n", key="guardar_produccion"):
                    try:
                        nuevo_registro = {
                            "fecha": pd.Timestamp(fecha_ayer),
                            "datos": datos_ayer.to_dict('records'),
                            "modulo": "SPE"
                        }
                        collection.insert_one(nuevo_registro)
                        st.success(f"‚úÖ Producci√≥n guardada exitosamente para la fecha: {fecha_ayer.strftime('%d/%m/%Y')}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error al guardar los datos: {str(e)}")
            elif datos_ayer is not None:
                st.warning(f"Ya existe un registro para el {ultima_fecha.strftime('%d/%m/%Y')}. Use el bot√≥n de resetear si necesita modificar.")

        with col2:
            if ultima_fecha_db:
                if st.button("üîÑ Resetear √∫ltima fecha", key="resetear_fecha"):
                    try:
                        collection.delete_many({
                            "modulo": "SPE",
                            "fecha": ultima_fecha_db
                        })
                        st.success("‚úÖ √öltima fecha eliminada correctamente")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error al resetear la √∫ltima fecha: {str(e)}")

    def _get_last_date_from_db(self, collection):
        """Obtener la √∫ltima fecha registrada en la base de datos."""
        fecha_actual = pd.Timestamp.now(tz='America/Lima').date()
        
        # Buscar el √∫ltimo registro que NO sea del d√≠a actual
        ultimo_registro = collection.find_one(
            {
                "modulo": "SPE",
                "fecha": {"$lt": pd.Timestamp(fecha_actual, tz='America/Lima')}
            }, 
            sort=[("fecha", -1)]
        )
        return ultimo_registro['fecha'] if ultimo_registro else None

    def render_pending_report(self, data):
        """Renderizar reporte de pendientes."""
        st.header("Reporte de Pendientes")

        # Mapeo de nombres de columnas con los nombres reales del Google Sheet
        COLUMNAS = {
            'EVALUADOR': 'EVALUADOR',
            'EXPEDIENTE': 'EXPEDIENTE',
            'ETAPA': 'ETAPA_EVALUACI√ìN',
            'ESTADO': 'ESTADO',
            'FECHA_INGRESO': 'FECHA _ INGRESO',
            'FECHA_TRABAJO': 'Fecha_Trabajo'
        }

        # Limpiar datos innecesarios
        data = data.drop(['Column 12', 'Column 13', 'Column 14', 'Column 15', 'Column 16', 'Column 17'], axis=1)

        # Filtrar solo expedientes pendientes (INICIADA o en blanco)
        data_filtrada = data[
            data[COLUMNAS['ETAPA']].isin(["", "INICIADA"]) | 
            data[COLUMNAS['ETAPA']].isna()
        ]

        # 1. TABLA DE EVALUADORES
        st.subheader("Pendientes por Evaluador")
        
        pivot_table = pd.pivot_table(
            data_filtrada,
            index=COLUMNAS['EVALUADOR'],
            values=COLUMNAS['EXPEDIENTE'],
            aggfunc='nunique',
            margins=True,
            margins_name='Total'
        )
        pivot_table.columns = ['Cantidad de Expedientes']

        # Ordenar excluyendo total
        total_row = pivot_table.loc[['Total']]
        rest_of_data = pivot_table.drop('Total')
        rest_of_data = rest_of_data.sort_values(by='Cantidad de Expedientes', ascending=False)
        pivot_table = pd.concat([rest_of_data, total_row])

        st.dataframe(
            pivot_table,
            use_container_width=True,
            height=400
        )

        # 2. GR√ÅFICO DE EVALUADORES
        if len(pivot_table) > 1:
            fig = px.bar(
                pivot_table.reset_index().iloc[:-1],
                x=COLUMNAS['EVALUADOR'],
                y='Cantidad de Expedientes',
                title="Distribuci√≥n de Expedientes Pendientes por Evaluador",
                text_auto=True
            )
            fig.update_traces(textposition='outside')
            st.plotly_chart(fig, use_container_width=True)

        # 3. TABLA DE ESTADOS
        st.subheader("Pendientes por Estado")
        
        data_iniciados = data_filtrada[data_filtrada[COLUMNAS['ETAPA']] == "INICIADA"]
        pivot_table_estado = pd.pivot_table(
            data_iniciados,
            index=COLUMNAS['ESTADO'],
            values=COLUMNAS['EXPEDIENTE'],
            aggfunc='nunique',
            margins=True,
            margins_name='Total'
        )
        pivot_table_estado.columns = ['Cantidad de Expedientes']

        # Ordenar excluyendo total
        total_row_estado = pivot_table_estado.loc[['Total']]
        rest_of_data_estado = pivot_table_estado.drop('Total')
        rest_of_data_estado = rest_of_data_estado.sort_values(by='Cantidad de Expedientes', ascending=False)
        pivot_table_estado = pd.concat([rest_of_data_estado, total_row_estado])

        st.dataframe(
            pivot_table_estado,
            use_container_width=True,
            height=400
        )

        # 4. GR√ÅFICO DE ESTADOS
        if len(pivot_table_estado) > 1:
            fig_estado = px.bar(
                pivot_table_estado.reset_index().iloc[:-1],
                x=COLUMNAS['ESTADO'],
                y='Cantidad de Expedientes',
                title="Distribuci√≥n de Expedientes Pendientes por Estado",
                text_auto=True
            )
            fig_estado.update_traces(textposition='outside')
            st.plotly_chart(fig_estado, use_container_width=True)

        # BOT√ìN DE DESCARGA AL FINAL
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            pivot_table.to_excel(writer, sheet_name='Expedientes_Por_Evaluador')
            pivot_table_estado.to_excel(writer, sheet_name='Expedientes_Por_Estado')
            
            detalle = data_filtrada[[
                COLUMNAS['EXPEDIENTE'], 
                COLUMNAS['EVALUADOR'], 
                COLUMNAS['ETAPA'],
                COLUMNAS['ESTADO'],
                COLUMNAS['FECHA_INGRESO']
            ]].sort_values([COLUMNAS['EVALUADOR'], COLUMNAS['FECHA_INGRESO']])
            detalle.to_excel(writer, sheet_name='Detalle_Expedientes', index=False)
        output.seek(0)

        st.download_button(
            label="Descargar Reporte Completo",
            data=output,
            file_name=f"reporte_expedientes_pendientes.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    def render_worked_report(self, data):
        """Renderizar reporte de expedientes trabajados."""
        st.header("Reporte de Expedientes Trabajados")

        # Mapeo de columnas
        COLUMNAS = {
            'EVALUADOR': 'EVALUADOR',
            'EXPEDIENTE': 'EXPEDIENTE',
            'FECHA_TRABAJO': 'Fecha_Trabajo'
        }

        # Mapeo de meses a espa√±ol
        MESES = {
            1: 'Enero', 2: 'Febrero', 3: 'Marzo', 4: 'Abril',
            5: 'Mayo', 6: 'Junio', 7: 'Julio', 8: 'Agosto',
            9: 'Septiembre', 10: 'Octubre', 11: 'Noviembre', 12: 'Diciembre'
        }

        # Convertir fecha de trabajo a datetime considerando formato dd/mm/yyyy
        data[COLUMNAS['FECHA_TRABAJO']] = pd.to_datetime(
            data[COLUMNAS['FECHA_TRABAJO']], 
            format='%d/%m/%Y',
            errors='coerce'
        )

        # Obtener mes anterior y mes actual
        fecha_actual = pd.Timestamp.now()
        mes_anterior = (fecha_actual - pd.DateOffset(months=1))
        
        # Funci√≥n auxiliar para procesar datos por mes
        def procesar_datos_mes(fecha, datos):
            nombre_mes = MESES[fecha.month]
            
            # Filtrar registros del mes
            datos_mes = datos[
                (datos[COLUMNAS['FECHA_TRABAJO']].dt.month == fecha.month) &
                (datos[COLUMNAS['FECHA_TRABAJO']].dt.year == fecha.year)
            ].copy()

            # Agrupar por evaluador
            stats = datos_mes.groupby(COLUMNAS['EVALUADOR']).agg({
                COLUMNAS['EXPEDIENTE']: 'count',  # Total expedientes
                COLUMNAS['FECHA_TRABAJO']: lambda x: x.dt.date.nunique()  # D√≠as √∫nicos trabajados
            }).reset_index()

            # Renombrar columnas
            stats.columns = ['EVALUADOR', 'CANT_EXPEDIENTES', 'DIAS_TRABAJADOS']

            # Calcular promedio
            stats['PROMEDIO'] = (stats['CANT_EXPEDIENTES'] / stats['DIAS_TRABAJADOS']).round(0)

            # Ordenar y agregar ranking
            stats_ordenado = stats.sort_values('CANT_EXPEDIENTES', ascending=False)
            stats_ordenado.index = range(1, len(stats_ordenado) + 1)

            return stats_ordenado, nombre_mes

        # Procesar mes anterior
        stats_mes_anterior, nombre_mes_anterior = procesar_datos_mes(mes_anterior, data)
        
        # Mostrar tabla mes anterior
        st.subheader(f"Expedientes Trabajados - {nombre_mes_anterior} {mes_anterior.year}")
        st.dataframe(
            stats_mes_anterior,
            use_container_width=True,
            height=400
        )

        # Procesar mes actual
        stats_mes_actual, nombre_mes_actual = procesar_datos_mes(fecha_actual, data)
        
        # Mostrar tabla mes actual
        st.subheader(f"Expedientes Trabajados - {nombre_mes_actual} {fecha_actual.year}")
        st.dataframe(
            stats_mes_actual,
            use_container_width=True,
            height=400
        )

        # Bot√≥n de descarga con ambos reportes
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Mes anterior
            stats_mes_anterior.to_excel(
                writer, 
                sheet_name=f'Estadisticas_{nombre_mes_anterior}_{mes_anterior.year}'
            )
            
            # Mes actual
            stats_mes_actual.to_excel(
                writer, 
                sheet_name=f'Estadisticas_{nombre_mes_actual}_{fecha_actual.year}'
            )
            
            # Detalles
            detalle = data[
                (data[COLUMNAS['FECHA_TRABAJO']].dt.month.isin([mes_anterior.month, fecha_actual.month])) &
                (data[COLUMNAS['FECHA_TRABAJO']].dt.year.isin([mes_anterior.year, fecha_actual.year]))
            ][[
                COLUMNAS['EXPEDIENTE'],
                COLUMNAS['EVALUADOR'],
                COLUMNAS['FECHA_TRABAJO']
            ]].sort_values([COLUMNAS['EVALUADOR'], COLUMNAS['FECHA_TRABAJO']])
            
            detalle.to_excel(
                writer, 
                sheet_name='Detalle_Expedientes',
                index=False
            )
        output.seek(0)

        st.download_button(
            label=f"Descargar Reporte {nombre_mes_anterior}-{nombre_mes_actual} {fecha_actual.year}",
            data=output,
            file_name=f"reporte_trabajados_{nombre_mes_anterior}_{nombre_mes_actual}_{fecha_actual.year}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    def _show_pending_summary(self, pendientes):
        """Mostrar resumen de pendientes."""
        pendientes_por_evaluador = pendientes.groupby('EVALASIGN').size().reset_index(name='Cantidad')
        pendientes_por_evaluador = pendientes_por_evaluador.sort_values(by='Cantidad', ascending=False)

        st.subheader("Cantidad de Pendientes por Evaluador")
        st.dataframe(pendientes_por_evaluador, use_container_width=True)

    def _show_pending_chart(self, pendientes):
        """Mostrar gr√°fico de pendientes."""
        pendientes_por_evaluador = pendientes.groupby('EVALASIGN').size().reset_index(name='Cantidad')
        
        st.subheader("Distribuci√≥n de Pendientes por Evaluador")
        fig = px.bar(
            pendientes_por_evaluador,
            x='EVALASIGN',
            y='Cantidad',
            title="Pendientes por Evaluador",
            labels={'EVALASIGN': 'Evaluador', 'Cantidad': 'N√∫mero de Expedientes'},
            text='Cantidad'
        )
        fig.update_traces(textposition='outside')
        st.plotly_chart(fig, use_container_width=True)

    def _offer_pending_download(self, pendientes):
        """Ofrecer descarga de reporte de pendientes."""
        pendientes_por_evaluador = pendientes.groupby('EVALASIGN').size().reset_index(name='Cantidad')
        
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            pendientes_por_evaluador.to_excel(writer, index=False, sheet_name='Pendientes')
            pendientes[['NumeroTramite', 'EVALASIGN', 'ETAPA_EVALUACION']].to_excel(
                writer, 
                index=False, 
                sheet_name='Detalle_Pendientes'
            )
        output.seek(0)

        st.download_button(
            label="Descargar Reporte de Pendientes",
            data=output,
            file_name="Pendientes_SPE.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        ) 

    def _migrate_evaluador_field(self, collection):
        """Migrar registros antiguos para usar 'EVALUADOR' en may√∫sculas."""
        try:
            # Buscar registros que usan 'evaluador' en min√∫sculas
            registros_antiguos = collection.find({
                "modulo": "SPE",
                "datos": {"$elemMatch": {"evaluador": {"$exists": True}}}
            })
            
            for registro in registros_antiguos:
                datos_actualizados = []
                for dato in registro['datos']:
                    if 'evaluador' in dato:
                        dato['EVALUADOR'] = dato.pop('evaluador')
                    datos_actualizados.append(dato)
                
                collection.update_one(
                    {"_id": registro["_id"]},
                    {"$set": {"datos": datos_actualizados}}
                )
                
        except Exception as e:
            st.error(f"Error al migrar datos: {str(e)}") 

    def crear_filtro_fecha_jerarquico(self, df, columna_fecha):
        """Crear filtro jer√°rquico para fechas (A√±o > Mes > D√≠a)."""
        fechas = pd.to_datetime(df[columna_fecha])
        
        # Obtener a√±os √∫nicos
        a√±os = sorted(fechas.dt.year.unique())
        a√±o_seleccionado = st.selectbox(
            f'A√±o ({columna_fecha})',
            options=['Todos'] + a√±os,
            key=f'a√±o_{columna_fecha}'
        )
        
        if a√±o_seleccionado != 'Todos':
            # Filtrar por a√±o
            mask_a√±o = fechas.dt.year == a√±o_seleccionado
            df_filtrado = df[mask_a√±o]
            
            # Obtener meses √∫nicos del a√±o seleccionado
            meses = sorted(fechas[mask_a√±o].dt.strftime('%m-%B').unique())
            mes_seleccionado = st.selectbox(
                f'Mes ({columna_fecha})',
                options=['Todos'] + meses,
                key=f'mes_{columna_fecha}'
            )
            
            if mes_seleccionado != 'Todos':
                # Filtrar por mes
                mes_num = mes_seleccionado.split('-')[0]
                mask_mes = fechas[mask_a√±o].dt.strftime('%m') == mes_num
                df_filtrado = df_filtrado[mask_mes]
                
                # Obtener d√≠as √∫nicos del mes seleccionado
                dias = sorted(fechas[mask_a√±o & mask_mes].dt.strftime('%d').unique())
                dia_seleccionado = st.selectbox(
                    f'D√≠a ({columna_fecha})',
                    options=['Todos'] + dias,
                    key=f'dia_{columna_fecha}'
                )
                
                if dia_seleccionado != 'Todos':
                    # Filtrar por d√≠a
                    mask_dia = fechas[mask_a√±o & mask_mes].dt.strftime('%d') == dia_seleccionado
                    df_filtrado = df_filtrado[mask_dia]
        else:
            df_filtrado = df
        
        return df_filtrado

    def render_dynamic_analysis(self, data):
        """Renderizar an√°lisis din√°mico tipo tabla din√°mica."""
        st.header("An√°lisis Din√°mico")

        # Eliminar columnas que empiezan con "Column"
        data = data[[col for col in data.columns if not col.startswith('Column')]]

        # Definir las columnas disponibles para an√°lisis
        COLUMNAS_DISPONIBLES = {
            'EVALUADOR': 'EVALUADOR',
            'EXPEDIENTE': 'EXPEDIENTE',
            'ETAPA': 'ETAPA_EVALUACI√ìN',
            'ESTADO': 'ESTADO',
            'PROCESO': 'PROCESO',
            'FECHA_INGRESO': 'FECHA _ INGRESO',
            'FECHA_TRABAJO': 'Fecha_Trabajo'
        }

        # Convertir columnas de fecha a datetime
        COLUMNAS_FECHA = ['FECHA_INGRESO', 'FECHA_TRABAJO']
        for col in COLUMNAS_FECHA:
            data[COLUMNAS_DISPONIBLES[col]] = pd.to_datetime(
                data[COLUMNAS_DISPONIBLES[col]], 
                format='%d/%m/%Y',
                dayfirst=True,
                errors='coerce'
            )

        # Crear columnas adicionales para fechas
        for col in COLUMNAS_FECHA:
            fecha_col = COLUMNAS_DISPONIBLES[col]
            data[f'{col}_A√ëO'] = data[fecha_col].dt.year
            data[f'{col}_MES'] = data[fecha_col].dt.strftime('%B-%Y')  # Nombre del mes y a√±o
            data[f'{col}_DIA'] = data[fecha_col].dt.strftime('%d-%B-%Y')  # D√≠a, mes y a√±o

        # Configuraci√≥n de AgGrid
        gb = GridOptionsBuilder.from_dataframe(data)

        # Configurar columnas principales
        for col in COLUMNAS_DISPONIBLES.values():
            gb.configure_column(col, filter=True, sorteable=True)

        # Configurar columnas de fecha con filtros especiales
        for col in COLUMNAS_FECHA:
            # Ocultar columna original de fecha
            gb.configure_column(COLUMNAS_DISPONIBLES[col], hide=True)
            
            # Configurar columnas de fecha desglosadas
            gb.configure_column(
                f'{col}_A√ëO',
                header_name=f'A√±o ({col})',
                filter='agNumberColumnFilter',
                sorteable=True
            )
            gb.configure_column(
                f'{col}_MES',
                header_name=f'Mes ({col})',
                filter='agTextColumnFilter',
                sorteable=True
            )
            gb.configure_column(
                f'{col}_DIA',
                header_name=f'D√≠a ({col})',
                filter='agTextColumnFilter',
                sorteable=True
            )

        # Configuraciones adicionales del grid
        gb.configure_default_column(
            groupable=True,
            value=True,
            enableRowGroup=True,
            enablePivot=True,
            enableValue=True
        )
        gb.configure_side_bar()
        gb.configure_selection(selection_mode='multiple', use_checkbox=True)
        gb.configure_grid_options(
            domLayout='normal',
            enableRangeSelection=True,
            enableCharts=True
        )

        grid_options = gb.build()

        # Mostrar grid interactivo
        st.subheader("Filtrado y An√°lisis Avanzado")
        AgGrid(
            data,
            grid_options,
            enable_enterprise_modules=True,
            update_mode='MODEL_CHANGED',
            data_return_mode='FILTERED_AND_SORTED',
            fit_columns_on_grid_load=False,
            theme='streamlit',
            height=500,
            allow_unsafe_jscode=True
        )

    def render_predictive_analysis(self, data):
        """Renderizar an√°lisis predictivo de ingresos usando ML y an√°lisis estad√≠stico avanzado."""
        st.header("An√°lisis Predictivo de Ingresos 2024")
        
        st.write("""
        Este an√°lisis muestra los patrones de ingreso de expedientes y realiza predicciones basadas en datos hist√≥ricos del 2024.
        Los insights generados ayudar√°n en la planificaci√≥n y distribuci√≥n de recursos.
        """)

        # Preparaci√≥n de datos
        data['FECHA _ INGRESO'] = pd.to_datetime(data['FECHA _ INGRESO'], format='%d/%m/%Y', errors='coerce')
        data = data.dropna(subset=['FECHA _ INGRESO'])
        data = data[data['FECHA _ INGRESO'].dt.year == 2024]

        # Enriquecer datos con caracter√≠sticas temporales
        data['dia_semana'] = data['FECHA _ INGRESO'].dt.dayofweek
        data['es_dia_habil'] = data['dia_semana'] < 5
        data['semana_mes'] = data['FECHA _ INGRESO'].dt.day.apply(lambda x: (x-1)//7 + 1)
        data['dia_mes'] = data['FECHA _ INGRESO'].dt.day
        data['mes'] = data['FECHA _ INGRESO'].dt.month

        # 1. An√°lisis de Patrones Temporales
        st.subheader("1. Patrones Temporales")
        st.write("""
        An√°lisis de la distribuci√≥n de ingresos seg√∫n diferentes per√≠odos de tiempo.
        Esto nos permite identificar los d√≠as y semanas con mayor carga de trabajo.
        """)
        
        col1, col2 = st.columns(2)

        with col1:
            # An√°lisis por d√≠a de la semana
            ingresos_dia_semana = data.groupby('dia_semana').size()
            dias = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']
            fig_dias = px.bar(
                x=dias,
                y=ingresos_dia_semana,
                title='Distribuci√≥n por D√≠a de Semana',
                labels={'x': 'D√≠a', 'y': 'Cantidad de Expedientes'}
            )
            st.plotly_chart(fig_dias, use_container_width=True)
            st.write("""
            Este gr√°fico muestra el volumen promedio de ingresos por cada d√≠a de la semana.
            Permite identificar los d√≠as con mayor carga laboral y planificar recursos acordemente.
            """)

        with col2:
            # An√°lisis por semana del mes
            ingresos_semana = data.groupby('semana_mes').size()
            fig_semanas = px.bar(
                x=['Semana ' + str(i) for i in range(1, 6)],
                y=ingresos_semana,
                title='Distribuci√≥n por Semana del Mes',
                labels={'x': 'Semana', 'y': 'Cantidad de Expedientes'}
            )
            st.plotly_chart(fig_semanas, use_container_width=True)
            st.write("""
            Muestra la distribuci√≥n de ingresos por semana del mes.
            Ayuda a identificar si existen patrones c√≠clicos en los ingresos mensuales.
            """)

        # 2. An√°lisis Estad√≠stico
        st.subheader("2. An√°lisis Estad√≠stico")
        st.write("""
        Comparaci√≥n estad√≠stica entre d√≠as h√°biles y no h√°biles.
        Los valores œÉ (sigma) indican la variabilidad de los datos: valores m√°s altos indican mayor variabilidad.
        """)

        # Calcular estad√≠sticas por tipo de d√≠a
        ingresos_por_tipo_dia = data.groupby(['FECHA _ INGRESO', 'es_dia_habil']).size().reset_index(name='cantidad')
        stats_dias = ingresos_por_tipo_dia.groupby('es_dia_habil').agg({
            'cantidad': ['mean', 'std']
        }).round(2)

        col1, col2 = st.columns(2)
        
        with col1:
            promedio_habil = stats_dias.loc[True, ('cantidad', 'mean')]
            std_habil = stats_dias.loc[True, ('cantidad', 'std')]
            st.metric(
                "Promedio D√≠as H√°biles",
                f"{promedio_habil:.0f}",
                f"¬±{std_habil:.0f} œÉ"
            )
        with col2:
            promedio_no_habil = stats_dias.loc[False, ('cantidad', 'mean')]
            std_no_habil = stats_dias.loc[False, ('cantidad', 'std')]
            st.metric(
                "Promedio D√≠as No H√°biles",
                f"{promedio_no_habil:.0f}",
                f"¬±{std_no_habil:.0f} œÉ"
            )

        # 3. An√°lisis de Tendencias y Predicciones
        st.subheader("3. Predicciones y Tendencias")
        st.write("""
        An√°lisis de la tendencia hist√≥rica y predicci√≥n de ingresos futuros.
        La l√≠nea punteada muestra la tendencia esperada basada en el comportamiento hist√≥rico.
        """)

        # Preparar datos para predicci√≥n con ajustes
        daily_counts = data.groupby('FECHA _ INGRESO').size().reset_index()
        daily_counts.columns = ['fecha', 'cantidad']
        
        # Asegurar que tenemos todas las fechas
        fecha_completa = pd.date_range(
            start=daily_counts['fecha'].min(),
            end=daily_counts['fecha'].max(),
            freq='D'
        )
        daily_counts = daily_counts.set_index('fecha').reindex(fecha_completa).fillna(0).reset_index()
        daily_counts.columns = ['fecha', 'cantidad']
        
        # Preparar datos para el modelo
        daily_counts['dias_transcurridos'] = (daily_counts['fecha'] - daily_counts['fecha'].min()).dt.days
        
        # Ajustar el modelo
        X = daily_counts['dias_transcurridos'].values.reshape(-1, 1)
        y = daily_counts['cantidad'].values
        poly = PolynomialFeatures(degree=2)
        X_poly = poly.fit_transform(X)
        model = LinearRegression()
        model.fit(X_poly, y)

        # Generar predicciones para los pr√≥ximos 30 d√≠as
        dias_futuros = 30
        X_future = np.linspace(X.min(), X.max() + dias_futuros, X.max() + dias_futuros).reshape(-1, 1)
        X_future_poly = poly.transform(X_future)
        y_pred = model.predict(X_future_poly)

        # Gr√°fico de tendencia y predicci√≥n
        fig_trend = go.Figure()
        
        # Datos reales
        fig_trend.add_trace(go.Scatter(
            x=daily_counts['fecha'],
            y=daily_counts['cantidad'],
            mode='markers',
            name='Datos reales',
            marker=dict(size=6)
        ))
        
        # Tendencia y predicci√≥n
        fechas_prediccion = pd.date_range(
            start=daily_counts['fecha'].min(),
            periods=len(y_pred),
            freq='D'
        )
        
        fig_trend.add_trace(go.Scatter(
            x=fechas_prediccion,
            y=y_pred,
            mode='lines',
            name='Tendencia y predicci√≥n',
            line=dict(dash='dash', color='red')
        ))
        
        fig_trend.update_layout(
            title='Tendencia y Predicci√≥n de Ingresos',
            xaxis_title='Fecha',
            yaxis_title='Cantidad de Expedientes',
            hovermode='x unified'
        )
        
        st.plotly_chart(fig_trend, use_container_width=True)
        
        st.write("""
        **Interpretaci√≥n del gr√°fico:**
        - Los puntos azules representan los ingresos diarios reales
        - La l√≠nea punteada roja muestra la tendencia y predicci√≥n
        - La predicci√≥n se extiende 30 d√≠as m√°s all√° de los datos actuales
        - Las √°reas donde la l√≠nea sube indican tendencia al alza, y viceversa
        """)

        # 4. Indicadores Clave y Alertas
        st.subheader("4. Indicadores Clave y Alertas")
        st.write("""
        M√©tricas importantes para monitorear el comportamiento de los ingresos:
        - **Tendencia √öltimos 7 d√≠as**: Compara el promedio de la √∫ltima semana con la semana anterior
        - **Volatilidad**: Indica qu√© tan variables son los ingresos (mayor % = m√°s variable)
        - **D√≠as At√≠picos**: D√≠as con ingresos inusualmente altos
        """)

        # Calcular indicadores
        tendencia_corto_plazo = (daily_counts['cantidad'].tail(7).mean() / 
                                daily_counts['cantidad'].tail(14).head(7).mean() - 1) * 100
        
        volatilidad = daily_counts['cantidad'].std() / daily_counts['cantidad'].mean() * 100
        
        # Calcular percentiles para detecci√≥n de anomal√≠as
        p25, p75 = np.percentile(daily_counts['cantidad'], [25, 75])
        iqr = p75 - p25
        limite_superior = p75 + 1.5 * iqr
        
        dias_atipicos = daily_counts[daily_counts['cantidad'] > limite_superior]
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "Tendencia √öltimos 7 d√≠as",
                f"{tendencia_corto_plazo:.1f}%",
                delta=tendencia_corto_plazo,
                delta_color="normal"
            )
        
        with col2:
            st.metric(
                "Volatilidad",
                f"{volatilidad:.1f}%",
                help="Coeficiente de variaci√≥n de los ingresos"
            )
        
        with col3:
            st.metric(
                "D√≠as At√≠picos",
                len(dias_atipicos),
                help="D√≠as con ingresos inusualmente altos"
            )

        # 5. Conclusiones y Recomendaciones
        st.subheader("5. Conclusiones y Recomendaciones")
        st.write("""
        Basado en el an√°lisis de los datos, se presentan las siguientes conclusiones
        y recomendaciones para optimizar la gesti√≥n de expedientes:
        """)

        conclusiones = []
        
        # An√°lisis de tendencia
        if tendencia_corto_plazo > 5:
            conclusiones.append("üìà Tendencia al alza significativa en los √∫ltimos 7 d√≠as.")
        elif tendencia_corto_plazo < -5:
            conclusiones.append("üìâ Tendencia a la baja significativa en los ÔøΩÔøΩltimos 7 d√≠as.")
        
        # An√°lisis de volatilidad
        if volatilidad > 50:
            conclusiones.append("‚ö†Ô∏è Alta volatilidad en los ingresos. Se recomienda planificaci√≥n flexible.")
        
        # Predicci√≥n pr√≥xima semana
        proxima_semana = model.predict(poly.transform([[X.max() + 7]]))[0]
        conclusiones.append(f"üîÆ Predicci√≥n para pr√≥xima semana: {proxima_semana:.0f} ingresos diarios en promedio.")
        
        # D√≠as cr√≠ticos
        dias_mas_carga = dias[ingresos_dia_semana.argmax()]
        conclusiones.append(f"üìä {dias_mas_carga} es el d√≠a con mayor volumen de ingresos.")

        # Mostrar conclusiones
        for conclusion in conclusiones:
            st.write(conclusion)

        # Recomendaciones basadas en an√°lisis
        st.subheader("Recomendaciones Operativas")
        
        recomendaciones = []
        if tendencia_corto_plazo > 5:
            recomendaciones.extend([
                "- Preparar recursos adicionales para manejar el incremento",
                "- Priorizar expedientes m√°s antiguos",
                "- Considerar redistribuci√≥n de carga entre evaluadores"
            ])
        else:
            recomendaciones.extend([
                "- Aprovechar para reducir backlog",
                "- Realizar capacitaciones y mejoras de proceso",
                "- Preparar estrategias para futuros incrementos"
            ])

        for rec in recomendaciones:
            st.write(rec)